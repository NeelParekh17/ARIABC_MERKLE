diff --git a/src/backend/access/heap/heapam.c b/src/backend/access/heap/heapam.c
index db289fb..0bb17ce 100644
--- a/src/backend/access/heap/heapam.c
+++ b/src/backend/access/heap/heapam.c
@@ -69,6 +69,7 @@
 #include "utils/snapmgr.h"
 #include "utils/spccache.h"
 #include "catalog/index.h"
+#include "catalog/pg_am_d.h"
 #include "access/htup_details.h"
 #include "bcdb/worker.h"
 
@@ -1850,6 +1851,25 @@ ReleaseBulkInsertStatePin(BulkInsertState bistate)
 
 void
 heap_apply_index(Relation relation, TupleTableSlot *slot, bool conflict_check, bool unique_check)
+{
+	heap_apply_index_phase(relation, slot, conflict_check, unique_check, HEAP_INDEX_ALL);
+}
+
+/*
+ * heap_apply_index_phase - insert into indexes with phase control.
+ *
+ * phase controls which indexes are processed:
+ *   HEAP_INDEX_ALL        - all indexes (default, same as heap_apply_index)
+ *   HEAP_INDEX_NO_MERKLE  - skip merkle indexes (for btree-only pass)
+ *   HEAP_INDEX_MERKLE_ONLY - only merkle indexes
+ *
+ * This split is needed for BCDB optimistic inserts: the btree unique
+ * check must succeed BEFORE any merkle XOR is applied, because
+ * subtransaction rollback does NOT undo merkle shared-buffer writes.
+ */
+void
+heap_apply_index_phase(Relation relation, TupleTableSlot *slot,
+					   bool conflict_check, bool unique_check, int phase)
 {
 	ListCell       *index_cell;
 
@@ -1862,9 +1882,25 @@ heap_apply_index(Relation relation, TupleTableSlot *slot, bool conflict_check, b
         Datum   	index_values[INDEX_MAX_KEYS];
         bool 		isNull[INDEX_MAX_KEYS];
         IndexUniqueCheck indexUniqueCheck;
+        bool        is_merkle;
 
         indexOid = index_cell->oid_value;
         indexRelation = RelationIdGetRelation(indexOid);
+
+        is_merkle = (indexRelation->rd_rel->relam == MERKLE_AM_OID);
+
+        /* Skip based on phase */
+        if (phase == HEAP_INDEX_NO_MERKLE && is_merkle)
+        {
+            RelationClose(indexRelation);
+            continue;
+        }
+        if (phase == HEAP_INDEX_MERKLE_ONLY && !is_merkle)
+        {
+            RelationClose(indexRelation);
+            continue;
+        }
+
         indexInfo = BuildIndexInfo(indexRelation);
 
 		if (unique_check && indexRelation->rd_index->indisunique)
diff --git a/src/backend/access/merkle/merkleutil.c b/src/backend/access/merkle/merkleutil.c
index 49a0d30..044d204 100644
--- a/src/backend/access/merkle/merkleutil.c
+++ b/src/backend/access/merkle/merkleutil.c
@@ -73,22 +73,32 @@ merkle_xact_callback(XactEvent event, void *arg)
             rel = try_relation_open(op->relid, RowExclusiveLock);
             if (rel != NULL)
             {
-                /* Read tree configuration from metadata */
+                /* Read validated tree configuration from metadata */
+                int         numPartitions;
                 int         leavesPerPartition;
                 int         nodesPerPartition;
+                int         totalNodes;
+                int         totalLeaves;
                 int         nodesPerPage;
-                Buffer      metabuf;
-                Page        metapage;
-                MerkleMetaPageData *meta;
-                
-                metabuf = ReadBuffer(rel, MERKLE_METAPAGE_BLKNO);
-                LockBuffer(metabuf, BUFFER_LOCK_SHARE);
-                metapage = BufferGetPage(metabuf);
-                meta = MerklePageGetMeta(metapage);
-                leavesPerPartition = meta->leavesPerPartition;
-                nodesPerPartition = meta->nodesPerPartition;
-                nodesPerPage = meta->nodesPerPage;
-                UnlockReleaseBuffer(metabuf);
+                int         numTreePages;
+				
+                merkle_read_meta(rel,
+                                 &numPartitions,
+                                 &leavesPerPartition,
+                                 &nodesPerPartition,
+                                 &totalNodes,
+                                 &totalLeaves,
+                                 &nodesPerPage,
+                                 &numTreePages);
+
+                if (op->leafId < 0 || op->leafId >= totalLeaves)
+                {
+                    ereport(WARNING,
+                            (errmsg("merkle_xact_callback: skipping invalid undo leafId %d (totalLeaves=%d)",
+                                    op->leafId, totalLeaves)));
+                    relation_close(rel, RowExclusiveLock);
+                    continue;
+                }
                 
                 ereport(NOTICE, (errmsg("merkle_xact_callback: Undoing op for leaf %d hash %s", 
                      op->leafId, merkle_hash_to_hex(&op->hash))));
@@ -106,9 +116,26 @@ merkle_xact_callback(XactEvent event, void *arg)
                     while (nodeInPartition > 0)
                     {
                         int         actualNodeIdx = nodeId - 1;
-                        int         pageNum = actualNodeIdx / nodesPerPage;
+                        int         pageNum;
                         int         idxInPage = actualNodeIdx % nodesPerPage;
-                        BlockNumber blkno = MERKLE_TREE_START_BLKNO + pageNum;
+                        BlockNumber blkno;
+
+                        if (actualNodeIdx < 0 || actualNodeIdx >= totalNodes)
+                        {
+                            ereport(WARNING,
+                                    (errmsg("merkle_xact_callback: invalid node index %d during undo", actualNodeIdx)));
+                            break;
+                        }
+
+                        pageNum = actualNodeIdx / nodesPerPage;
+                        if (pageNum < 0 || pageNum >= numTreePages)
+                        {
+                            ereport(WARNING,
+                                    (errmsg("merkle_xact_callback: invalid page number %d during undo", pageNum)));
+                            break;
+                        }
+
+                        blkno = MERKLE_TREE_START_BLKNO + pageNum;
                         
                         /* Switch pages if needed */
                         if ((int)blkno != currentPageBlkno)
@@ -340,6 +367,64 @@ merkle_compute_row_hash(Relation heapRel, ItemPointer tid, MerkleHash *result)
     ExecDropSingleTupleTableSlot(slot);
 }
 
+/*
+ * merkle_compute_slot_hash() - Compute integrity hash from an already-fetched slot.
+ *
+ * This variant avoids heap re-fetch by hashing the visible values currently
+ * present in `slot`. It is used when we must defer Merkle mutation until after
+ * heap operation success, while still hashing the OLD row image.
+ */
+void
+merkle_compute_slot_hash(Relation heapRel, TupleTableSlot *slot, MerkleHash *result)
+{
+    TupleDesc       tupdesc;
+    StringInfoData  buf;
+    blake3_hasher   hasher;
+    int             i;
+
+    if (slot == NULL || TTS_EMPTY(slot))
+    {
+        merkle_hash_zero(result);
+        return;
+    }
+
+    tupdesc = RelationGetDescr(heapRel);
+    initStringInfo(&buf);
+
+    for (i = 0; i < tupdesc->natts; i++)
+    {
+        Form_pg_attribute attr = TupleDescAttr(tupdesc, i);
+        Datum       val;
+        bool        isnull;
+        Oid         typoutput;
+        bool        typIsVarlena;
+        char       *str;
+
+        if (attr->attisdropped)
+            continue;
+
+        val = slot_getattr(slot, i + 1, &isnull);
+
+        if (isnull)
+        {
+            appendStringInfoString(&buf, "*null*");
+        }
+        else
+        {
+            getTypeOutputInfo(attr->atttypid, &typoutput, &typIsVarlena);
+            str = OidOutputFunctionCall(typoutput, val);
+            appendStringInfo(&buf, "*%s", str);
+            pfree(str);
+        }
+    }
+
+    blake3_hasher_init(&hasher);
+    blake3_hasher_update(&hasher, buf.data, buf.len);
+    blake3_hasher_finalize(&hasher, result->data, MERKLE_HASH_BYTES);
+
+    pfree(buf.data);
+}
+
 /*
  * merkle_compute_partition_id_single() - Internal helper for single-key partition
  *
@@ -352,6 +437,13 @@ merkle_compute_partition_id_single(Datum key, Oid keytype, int numLeaves)
     int64   keyval;
     int     pid;
     
+    /* Safety check: prevent division by zero */
+    if (numLeaves <= 0)
+    {
+        elog(WARNING, "merkle_compute_partition_id_single: invalid numLeaves=%d, returning 0", numLeaves);
+        return 0;
+    }
+    
     /*
      * Convert key to integer for partition calculation.
      * For non-integer types, we hash the key value.
@@ -423,6 +515,13 @@ merkle_compute_partition_id(Datum *values, bool *isnull, int nkeys,
     char       *p;
     int         i;
     
+    /* Safety check: prevent division by zero */
+    if (numLeaves <= 0)
+    {
+        elog(WARNING, "merkle_compute_partition_id: invalid numLeaves=%d, returning 0", numLeaves);
+        return 0;
+    }
+    
     /* If only one key, use the optimized single-key path */
     if (nkeys == 1)
     {
@@ -487,12 +586,16 @@ merkle_compute_partition_id(Datum *values, bool *isnull, int nkeys,
 void
 merkle_update_tree_path(Relation indexRel, int leafId, MerkleHash *hash, bool isXorIn)
 {
+    int         numPartitions;
     int         partitionId;
     int         nodeInPartition;
     int         nodeId;
     int         leavesPerPartition;
     int         nodesPerPartition;
+    int         totalNodes;
+    int         totalLeaves;
     int         nodesPerPage;
+    int         numTreePages;
     int         currentPageBlkno = -1;
     Buffer      buf = InvalidBuffer;
     Page        page = NULL;
@@ -501,8 +604,29 @@ merkle_update_tree_path(Relation indexRel, int leafId, MerkleHash *hash, bool is
     MerklePendingOp *op;
     
     /* Read tree configuration from metadata */
-    merkle_read_meta(indexRel, NULL, &leavesPerPartition, &nodesPerPartition, NULL, NULL,
-                          &nodesPerPage, NULL);
+    merkle_read_meta(indexRel,
+                     &numPartitions,
+                     &leavesPerPartition,
+                     &nodesPerPartition,
+                     &totalNodes,
+                     &totalLeaves,
+                     &nodesPerPage,
+                     &numTreePages);
+    
+    /* Safety check: prevent division by zero if metadata is invalid */
+    if (leavesPerPartition <= 0)
+    {
+        elog(WARNING, "merkle_update_tree_path: invalid leavesPerPartition=%d, ignoring update", leavesPerPartition);
+        return;
+    }
+
+    if (leafId < 0 || leafId >= totalLeaves)
+    {
+        ereport(ERROR,
+                (errcode(ERRCODE_INDEX_CORRUPTED),
+                 errmsg("merkle_update_tree_path: leafId %d out of range [0,%d)",
+                        leafId, totalLeaves)));
+    }
     
     /*
      * Register transaction callback if not done yet.
@@ -538,9 +662,28 @@ merkle_update_tree_path(Relation indexRel, int leafId, MerkleHash *hash, bool is
     while (nodeInPartition > 0)
     {
         int         actualNodeIdx = nodeId - 1;  /* 0-based index */
-        int         pageNum = actualNodeIdx / nodesPerPage;
+        int         pageNum;
         int         idxInPage = actualNodeIdx % nodesPerPage;
-        BlockNumber blkno = MERKLE_TREE_START_BLKNO + pageNum;
+        BlockNumber blkno;
+
+        if (actualNodeIdx < 0 || actualNodeIdx >= totalNodes)
+        {
+            ereport(ERROR,
+                    (errcode(ERRCODE_INDEX_CORRUPTED),
+                     errmsg("merkle_update_tree_path: node index %d out of range [0,%d)",
+                            actualNodeIdx, totalNodes)));
+        }
+
+        pageNum = actualNodeIdx / nodesPerPage;
+        if (pageNum < 0 || pageNum >= numTreePages)
+        {
+            ereport(ERROR,
+                    (errcode(ERRCODE_INDEX_CORRUPTED),
+                     errmsg("merkle_update_tree_path: page number %d out of range [0,%d)",
+                            pageNum, numTreePages)));
+        }
+
+        blkno = MERKLE_TREE_START_BLKNO + pageNum;
         
         /* Switch pages if needed */
         if ((int)blkno != currentPageBlkno)
@@ -602,6 +745,19 @@ merkle_read_meta(Relation indexRel, int *numPartitions, int *leavesPerPartition,
     page = BufferGetPage(buf);
     meta = MerklePageGetMeta(page);
     
+    /* Validate metadata integrity - corrupted/uninitialized values cause crashes */
+    if (meta->numPartitions <= 0 || meta->leavesPerPartition <= 0)
+    {
+        UnlockReleaseBuffer(buf);
+        ereport(ERROR,
+                (errcode(ERRCODE_INDEX_CORRUPTED),
+                 errmsg("Merkle index \"%s\" has corrupted metadata",
+                        RelationGetRelationName(indexRel)),
+                 errdetail("numPartitions=%d, leavesPerPartition=%d",
+                           meta->numPartitions, meta->leavesPerPartition),
+                 errhint("Try REINDEXing the Merkle index.")));
+    }
+    
     /* Read values from metadata */
     if (numPartitions)
         *numPartitions = meta->numPartitions;
diff --git a/src/backend/bcdb/shm_block.c b/src/backend/bcdb/shm_block.c
index 1545e58..12ec52e 100644
--- a/src/backend/bcdb/shm_block.c
+++ b/src/backend/bcdb/shm_block.c
@@ -73,8 +73,12 @@ void set_last_committed_txid( BCDBShmXact *tx)
 {
     //BCBlock* blk = get_block_by_id( tx->block_id_committed, false);
     BCBlock* blk = get_block_by_id(1, true);
+    /* Atomic counter update with spinlock to prevent race conditions */
+    SpinLockAcquire(block_pool_lock);
     blk->last_committed_tx_id = tx->tx_id;
     block_meta->num_committed = tx->tx_id;
+    pg_write_barrier();  /* Ensure write ordering across processes */
+    SpinLockRelease(block_pool_lock);
 #if SAFEDBG2
     printf("safeDbg %s : %s: %d  blk %x txid= %d\n",
               __FILE__, __FUNCTION__, __LINE__, blk, block_meta->num_committed);
@@ -135,9 +139,15 @@ BCTxID get_last_committed_txid(BCDBShmXact *tx)
 {
     //BCBlock* blk = get_block_by_id( tx->block_id_committed, false);
     BCBlock* blk = get_block_by_id(1, false);
+    BCTxID result;
+    /* Atomic counter read with spinlock to prevent torn reads */
+    SpinLockAcquire(block_pool_lock);
+    pg_read_barrier();  /* Ensure read ordering across processes */
+    result = blk->last_committed_tx_id;
+    SpinLockRelease(block_pool_lock);
     //printf("ariaMyDbg %s : %s: %d bid 1, blk %x\n",
               //__FILE__, __FUNCTION__, __LINE__, blk);
-    return blk->last_committed_tx_id ;
+    return result;
 }
 
 BCBlock*
diff --git a/src/backend/bcdb/shm_transaction.c b/src/backend/bcdb/shm_transaction.c
index 5fd1606..71ada71 100644
--- a/src/backend/bcdb/shm_transaction.c
+++ b/src/backend/bcdb/shm_transaction.c
@@ -87,7 +87,9 @@ create_tx(char *hash, char *sql, BCTxID tx_id, BCBlockID snapshot_block, int iso
     tx = hash_search(tx_pool, hash, HASH_ENTER, &found);
     if (found)
     {
-        printf("safeDB %s : %s: %d \n", __FILE__, __FUNCTION__, __LINE__ );
+#if SAFEDBG2
+        printf("safeDB %s : %s: %d duplicate hash %s\n", __FILE__, __FUNCTION__, __LINE__, hash);
+#endif
         ereport(DEBUG3,
             (errmsg("[ZL] transaction (%s) exists", hash)));
         SpinLockRelease(tx_pool_lock);
@@ -774,6 +776,7 @@ store_optim_update(TupleTableSlot* slot, ItemPointer old_tid)
     write_entry->old_tid = *old_tid;
     write_entry->slot = clone_slot(slot);
     write_entry->cid = GetCurrentCommandId(true);
+    write_entry->relOid = InvalidOid;
     SIMPLEQ_INSERT_TAIL(&activeTx->optim_write_list, write_entry, link);
     MemoryContextSwitchTo(old_context);
 #if SAFEDBG1
@@ -794,20 +797,87 @@ store_optim_insert(TupleTableSlot* slot)
     write_entry->operation = CMD_INSERT;
     write_entry->slot = clone_slot(slot);
     write_entry->cid = GetCurrentCommandId(true);
+    write_entry->relOid = InvalidOid;
     SIMPLEQ_INSERT_TAIL(&activeTx->optim_write_list, write_entry, link);
     MemoryContextSwitchTo(old_context);
 }
 
 void
+store_optim_delete(Oid relOid, ItemPointer tupleid)
+{
+    OptimWriteEntry *write_entry;
+    MemoryContext    old_context;
+    DEBUGMSG("[ZL] tx %s storing delete (rel: %d)", activeTx->hash, relOid);
+    old_context = MemoryContextSwitchTo(bcdb_tx_context);
+    write_entry = palloc(sizeof(OptimWriteEntry));
+    write_entry->operation = CMD_DELETE;
+    write_entry->slot = NULL;           /* no slot needed for DELETE */
+    write_entry->old_tid = *tupleid;
+    write_entry->relOid = relOid;
+    write_entry->cid = GetCurrentCommandId(true);
+    SIMPLEQ_INSERT_TAIL(&activeTx->optim_write_list, write_entry, link);
+    MemoryContextSwitchTo(old_context);
+}
+
+bool
 apply_optim_insert(TupleTableSlot* slot, CommandId cid)
 {
     Relation relation = RelationIdGetRelation(slot->tts_tableOid);
+    MemoryContext old_context = CurrentMemoryContext;
+    ResourceOwner old_owner = CurrentResourceOwner;
+    bool insert_ok = false;
 
     DEBUGMSG("[ZL] tx %s applying optim insert (rel: %d)", activeTx->hash, relation->rd_id);
-    table_tuple_insert(relation, slot, cid, 0, NULL);
 
-    heap_apply_index(relation, slot, true, true);
+    /*
+     * Two-phase index insertion to protect merkle tree integrity.
+     *
+     * Phase 1 (subtransaction): heap insert + btree indexes only.
+     *   If btree raises duplicate-key ERROR, the subtransaction is
+     *   rolled back cleanly — no merkle state was touched.
+     *
+     * Phase 2 (parent transaction): merkle index only.
+     *   Only runs if phase 1 succeeded, so the XOR into the merkle
+     *   tree is guaranteed to correspond to an actually-inserted row.
+     *
+     * Why: subtransaction rollback does NOT undo merkle XOR writes
+     * in shared buffers (they are direct page modifications, not
+     * undoable via CLOG abort). Processing merkle before btree in
+     * a single heap_apply_index call leaves phantom XOR entries
+     * when the btree unique check subsequently fails.
+     */
+
+    /* Phase 1: heap + btree (no merkle) inside subtransaction */
+    BeginInternalSubTransaction("bcdb_insert");
+    PG_TRY();
+    {
+        table_tuple_insert(relation, slot, cid, 0, NULL);
+        heap_apply_index_phase(relation, slot, true, true, HEAP_INDEX_NO_MERKLE);
+        /* Btree unique check passed — merge subtxn into parent */
+        ReleaseCurrentSubTransaction();
+        MemoryContextSwitchTo(old_context);
+        CurrentResourceOwner = old_owner;
+        insert_ok = true;
+    }
+    PG_CATCH();
+    {
+        /* Duplicate key or other error — rollback subtxn.
+         * Heap insert and btree entries are undone. Merkle is untouched. */
+        MemoryContextSwitchTo(old_context);
+        CurrentResourceOwner = old_owner;
+        RollbackAndReleaseCurrentSubTransaction();
+        FlushErrorState();
+    }
+    PG_END_TRY();
+
+    /* Phase 2: merkle index — only if phase 1 committed */
+    if (insert_ok)
+    {
+        heap_apply_index_phase(relation, slot, false, false, HEAP_INDEX_MERKLE_ONLY);
+    }
+
     RelationClose(relation);
+    return insert_ok;
 }
 
 void
@@ -818,8 +888,19 @@ apply_optim_update(ItemPointer tid, TupleTableSlot* slot, CommandId cid)
     LockTupleMode lockmode;
     bool update_indexes;
     Relation relation = RelationIdGetRelation(slot->tts_tableOid);
-    List *indexList;
+    List *indexList = NIL;
     ListCell *lc;
+    TupleTableSlot *oldSlot = NULL;
+    MerkleHash oldHash;
+    bool hasOldHash = false;
+    int pendingCount = 0;
+    int pendingCapacity = 0;
+    typedef struct PendingMerkleUpdate
+    {
+        Oid indexOid;
+        int partitionId;
+    } PendingMerkleUpdate;
+    PendingMerkleUpdate *pending = NULL;
 
     DEBUGMSG("[ZL] tx %s applying optim update (%d %d %d)", activeTx->hash, relation->rd_id, *(int*)&tid->ip_blkid, (int)tid->ip_posid);
 #if SAFEDBG1
@@ -827,102 +908,56 @@ apply_optim_update(ItemPointer tid, TupleTableSlot* slot, CommandId cid)
             __FILE__, __FUNCTION__, __LINE__ , TM_Ok, activeTx->tx_id, cid );
 #endif
     
-    /*
-     * MERKLE INDEX FIX: XOR OUT the old tuple's hash before UPDATE.
-     * This ensures Merkle trees stay consistent during optimistic writes.
-     * The NEW hash will be XOR'd IN by heap_apply_index() below.
-     * 
-     * Note: ItemPointerIsValid only checks ip_posid != 0, NOT the block number!
-     * We must also check for InvalidBlockNumber (0xFFFFFFFF).
-     */
     if (enable_merkle_index && ItemPointerIsValid(tid) &&
         ItemPointerGetBlockNumberNoCheck(tid) != InvalidBlockNumber)
     {
-        indexList = RelationGetIndexList(relation);
-        foreach(lc, indexList)
+        oldSlot = table_slot_create(relation, NULL);
+        if (!table_tuple_fetch_row_version(relation, tid, SnapshotSelf, oldSlot))
         {
-            Oid indexOid = lfirst_oid(lc);
-            Relation indexRel = index_open(indexOid, RowExclusiveLock);
-            
-            if (indexRel->rd_rel->relam == MERKLE_AM_OID)
+            RelationClose(relation);
+            ereport(ERROR,
+                    (errcode(ERRCODE_T_R_SERIALIZATION_FAILURE),
+                     errmsg("tx %s doomed because old row image was not fetchable", activeTx->hash)));
+        }
+
+        merkle_compute_slot_hash(relation, oldSlot, &oldHash);
+        hasOldHash = !merkle_hash_is_zero(&oldHash);
+
+        if (hasOldHash)
+        {
+            indexList = RelationGetIndexList(relation);
+            pendingCapacity = list_length(indexList);
+            if (pendingCapacity > 0)
+                pending = palloc0(sizeof(PendingMerkleUpdate) * pendingCapacity);
+
+            foreach(lc, indexList)
             {
-                MerkleHash hash;
-                TupleDesc indexTupdesc;
-                int partitionId;
-                int nkeys;
-                int16 *indkey;
-                Datum *keyValues;
-                bool *keyNulls;
-                int i;
-                int totalLeaves;
-                TupleTableSlot *oldSlot;
-                
-                indexTupdesc = RelationGetDescr(indexRel);
-                nkeys = indexRel->rd_index->indnkeyatts;
-                indkey = indexRel->rd_index->indkey.values;
-                
-                merkle_read_meta(indexRel, NULL, NULL, NULL, NULL, &totalLeaves, NULL, NULL);
-                
-                keyValues = (Datum *) palloc(nkeys * sizeof(Datum));
-                keyNulls = (bool *) palloc(nkeys * sizeof(bool));
-                
-                /* Try to fetch the OLD tuple first to see if it exists */
-                oldSlot = table_slot_create(relation, NULL);
-                
-                /* 
-                 * Wrap in PG_TRY to ensure oldSlot is dropped even if 
-                 * merkle_compute_row_hash or other functions throw an error.
-                 */
-                PG_TRY();
-                {
-                    if (table_tuple_fetch_row_version(relation, tid, SnapshotSelf, oldSlot))
-                    {
-                        /* Compute hash of the OLD row being replaced */
-                        merkle_compute_row_hash(relation, tid, &hash);
-                        
-                        if (!merkle_hash_is_zero(&hash))
-                        {
-                            /* Get key values from old tuple to compute partition ID */
-                            for (i = 0; i < nkeys; i++)
-                            {
-                                int heapAttr = indkey[i];
-                                keyValues[i] = slot_getattr(oldSlot, heapAttr, &keyNulls[i]);
-                            }
-                            
-                            partitionId = merkle_compute_partition_id(keyValues, keyNulls,
-                                                                             nkeys, indexTupdesc,
-                                                                             totalLeaves);
-                            
-                            /* XOR OUT the old hash */
-                            merkle_update_tree_path(indexRel, partitionId, &hash, false);
-                        }
-                    }
-                }
-                PG_CATCH();
+                Oid indexOid = lfirst_oid(lc);
+                Relation indexRel = index_open(indexOid, RowExclusiveLock);
+
+                if (indexRel->rd_rel->relam == MERKLE_AM_OID)
                 {
-                    ErrorData *edata;
-                    
-                    /* Clean up slot before re-throwing */
-                    ExecDropSingleTupleTableSlot(oldSlot);
-                    
-                    /* Log the error for debugging */
-                    edata = CopyErrorData();
-                    elog(DEBUG1, "apply_optim_update: Merkle error: %s", edata->message);
-                    FreeErrorData(edata);
-                    
-                    PG_RE_THROW();
+                    IndexInfo  *indexInfo;
+                    Datum       values[INDEX_MAX_KEYS];
+                    bool        isnull[INDEX_MAX_KEYS];
+                    int         totalLeaves;
+
+                    indexInfo = BuildIndexInfo(indexRel);
+                    FormIndexDatum(indexInfo, oldSlot, NULL, values, isnull);
+                    merkle_read_meta(indexRel, NULL, NULL, NULL, NULL, &totalLeaves, NULL, NULL);
+
+                    pending[pendingCount].indexOid = indexOid;
+                    pending[pendingCount].partitionId =
+                        merkle_compute_partition_id(values, isnull,
+                                                    indexInfo->ii_NumIndexKeyAttrs,
+                                                    RelationGetDescr(indexRel),
+                                                    totalLeaves);
+                    pendingCount++;
                 }
-                PG_END_TRY();
-                
-                ExecDropSingleTupleTableSlot(oldSlot);
-                
-                pfree(keyValues);
-                pfree(keyNulls);
+
+                index_close(indexRel, RowExclusiveLock);
             }
-            
-            index_close(indexRel, RowExclusiveLock);
         }
-        list_free(indexList);
     }
     
     result = table_tuple_update(relation, tid, slot,
@@ -934,6 +969,12 @@ apply_optim_update(ItemPointer tid, TupleTableSlot* slot, CommandId cid)
 
     if (result != TM_Ok)
     {
+        if (oldSlot)
+            ExecDropSingleTupleTableSlot(oldSlot);
+        if (indexList)
+            list_free(indexList);
+        if (pending)
+            pfree(pending);
         RelationClose(relation);
 #if SAFEDBG1
         printf("safeDB %s : %s: %d ret %d tx %d tx %s doomed because of ww-conflict \n",
@@ -946,8 +987,20 @@ apply_optim_update(ItemPointer tid, TupleTableSlot* slot, CommandId cid)
                  errmsg("tx %s doomed because of ww-conflict", activeTx->hash)));
     }
 
-    else if (update_indexes)
-        heap_apply_index(relation, slot, false, false);
+    if (hasOldHash)
+    {
+        int i;
+        for (i = 0; i < pendingCount; i++)
+        {
+            Relation indexRel = index_open(pending[i].indexOid, RowExclusiveLock);
+            if (indexRel->rd_rel->relam == MERKLE_AM_OID)
+                merkle_update_tree_path(indexRel, pending[i].partitionId, &oldHash, false);
+            index_close(indexRel, RowExclusiveLock);
+        }
+    }
+
+    if (update_indexes)
+        heap_apply_index_phase(relation, slot, false, false, HEAP_INDEX_NO_MERKLE);
 
     /*
      * MERKLE HOT-UPDATE FIX: If update_indexes is false (HOT update),
@@ -960,8 +1013,14 @@ apply_optim_update(ItemPointer tid, TupleTableSlot* slot, CommandId cid)
      * where 'id' is the indexed column) silently drop the row's hash
      * contribution from the Merkle tree, causing merkle_verify() failures.
      */
-    if (!update_indexes)
-        ExecInsertMerkleIndexes(relation, slot);
+    ExecInsertMerkleIndexes(relation, slot);
+
+    if (oldSlot)
+        ExecDropSingleTupleTableSlot(oldSlot);
+    if (indexList)
+        list_free(indexList);
+    if (pending)
+        pfree(pending);
 
     ExecDropSingleTupleTableSlot(slot);
 
@@ -969,9 +1028,142 @@ apply_optim_update(ItemPointer tid, TupleTableSlot* slot, CommandId cid)
 }
 
 void
+apply_optim_delete(Oid relOid, ItemPointer tupleid, CommandId cid)
+{
+    Relation relation = RelationIdGetRelation(relOid);
+    TM_FailureData tmfd;
+    TM_Result result;
+    TupleTableSlot *oldSlot = NULL;
+    List *indexList = NIL;
+    ListCell *lc;
+    MerkleHash oldHash;
+    bool hasOldHash = false;
+    int pendingCount = 0;
+    int pendingCapacity = 0;
+    typedef struct PendingMerkleDelete
+    {
+        Oid indexOid;
+        int partitionId;
+    } PendingMerkleDelete;
+    PendingMerkleDelete *pending = NULL;
+
+    DEBUGMSG("[ZL] tx %s applying optim delete (rel: %d)", activeTx->hash, relOid);
+
+    if (!enable_merkle_index)
+    {
+        result = table_tuple_delete(relation, tupleid,
+                           cid,
+                           InvalidSnapshot,
+                           InvalidSnapshot,
+                           false,
+                           &tmfd,
+                           false);
+
+        if (result != TM_Ok)
+        {
+            RelationClose(relation);
+            ereport(ERROR,
+                    (errcode(ERRCODE_T_R_SERIALIZATION_FAILURE),
+                     errmsg("tx %s doomed because of delete conflict", activeTx->hash)));
+        }
+
+        RelationClose(relation);
+        return;
+    }
+
+    oldSlot = table_slot_create(relation, NULL);
+    if (table_tuple_fetch_row_version(relation, tupleid, SnapshotSelf, oldSlot))
+    {
+        merkle_compute_slot_hash(relation, oldSlot, &oldHash);
+        hasOldHash = !merkle_hash_is_zero(&oldHash);
+
+        if (hasOldHash)
+        {
+            indexList = RelationGetIndexList(relation);
+            pendingCapacity = list_length(indexList);
+            if (pendingCapacity > 0)
+                pending = palloc0(sizeof(PendingMerkleDelete) * pendingCapacity);
+
+            foreach(lc, indexList)
+            {
+                Oid indexOid = lfirst_oid(lc);
+                Relation indexRel = index_open(indexOid, RowExclusiveLock);
+
+                if (indexRel->rd_rel->relam == MERKLE_AM_OID)
+                {
+                    IndexInfo  *indexInfo;
+                    Datum       values[INDEX_MAX_KEYS];
+                    bool        isnull[INDEX_MAX_KEYS];
+                    int         totalLeaves;
+
+                    indexInfo = BuildIndexInfo(indexRel);
+                    FormIndexDatum(indexInfo, oldSlot, NULL, values, isnull);
+                    merkle_read_meta(indexRel, NULL, NULL, NULL, NULL, &totalLeaves, NULL, NULL);
+
+                    pending[pendingCount].indexOid = indexOid;
+                    pending[pendingCount].partitionId =
+                        merkle_compute_partition_id(values, isnull,
+                                                    indexInfo->ii_NumIndexKeyAttrs,
+                                                    RelationGetDescr(indexRel),
+                                                    totalLeaves);
+                    pendingCount++;
+                }
+
+                index_close(indexRel, RowExclusiveLock);
+            }
+        }
+    }
+
+    /* Now delete the heap tuple */
+    result = table_tuple_delete(relation, tupleid,
+                       cid,
+                       InvalidSnapshot,
+                       InvalidSnapshot,
+                       false, /* do not wait for commit */
+                       &tmfd,
+                       false);
+
+    if (result != TM_Ok)
+    {
+        if (oldSlot)
+            ExecDropSingleTupleTableSlot(oldSlot);
+        if (indexList)
+            list_free(indexList);
+        if (pending)
+            pfree(pending);
+        RelationClose(relation);
+        ereport(ERROR,
+                (errcode(ERRCODE_T_R_SERIALIZATION_FAILURE),
+                 errmsg("tx %s doomed because of delete conflict", activeTx->hash)));
+    }
+
+    if (hasOldHash)
+    {
+        int i;
+        for (i = 0; i < pendingCount; i++)
+        {
+            Relation indexRel = index_open(pending[i].indexOid, RowExclusiveLock);
+            if (indexRel->rd_rel->relam == MERKLE_AM_OID)
+                merkle_update_tree_path(indexRel, pending[i].partitionId, &oldHash, false);
+            index_close(indexRel, RowExclusiveLock);
+        }
+    }
+
+    if (oldSlot)
+        ExecDropSingleTupleTableSlot(oldSlot);
+    if (indexList)
+        list_free(indexList);
+    if (pending)
+        pfree(pending);
+
+    RelationClose(relation);
+}
+
+bool
 apply_optim_writes(void)
 {
     OptimWriteEntry *write_entry;
+
     while ((write_entry = SIMPLEQ_FIRST(&activeTx->optim_write_list)))
     {
         switch (write_entry->operation)
@@ -980,21 +1172,31 @@ apply_optim_writes(void)
                 apply_optim_update(&write_entry->old_tid, write_entry->slot, write_entry->cid);
                 break;
             case CMD_INSERT:
-                apply_optim_insert(write_entry->slot, write_entry->cid);
-                /*
-                 * Clean up the cloned slot after insert.
-                 * apply_optim_update handles its own slot cleanup internally,
-                 * but apply_optim_insert does not, so we must do it here.
-                 * Without this, the slot's TupleDesc and any pinned resources
-                 * would leak on every INSERT through the BCDB worker.
-                 */
+                if (!apply_optim_insert(write_entry->slot, write_entry->cid))
+                {
+                    /*
+                     * INSERT failed (duplicate key).  This happens when
+                     * tx_id assignment doesn't preserve workload line order:
+                     * a DELETE-INSERT pair for the same key gets swapped so
+                     * the INSERT runs first and finds the original row still
+                     * present.  Signal failure so the worker retries the
+                     * whole transaction with a fresh snapshot.
+                     */
+                    ExecDropSingleTupleTableSlot(write_entry->slot);
+                    SIMPLEQ_REMOVE_HEAD(&activeTx->optim_write_list, link);
+                    return false;
+                }
                 ExecDropSingleTupleTableSlot(write_entry->slot);
                 break;
+            case CMD_DELETE:
+                apply_optim_delete(write_entry->relOid, &write_entry->old_tid, write_entry->cid);
+                break;
             default:
                 ereport(ERROR, (errmsg("[ZL] tx %s applying unknown operation", activeTx->hash)));
         }
         SIMPLEQ_REMOVE_HEAD(&activeTx->optim_write_list, link);
     }
+    return true;
 }
 
 bool
@@ -1041,10 +1243,11 @@ static int cc2Count = 0;
     LIST_FOREACH(record, &ws_table_record, link)
     {
         // ws_table_check
-        if (ws_table_checkDT( &record->tag))
-	printf("safeDB %s : %s: %d tx %s %d conflict due to waw \n", 
+        if (ws_table_checkDT( &record->tag)) {
+	    printf("safeDB %s : %s: %d tx %s %d conflict due to waw \n", 
 		__FILE__, __FUNCTION__, __LINE__ ,  activeTx->hash, activeTx->tx_id);
 	    return 1;
+	}
     		//ereport(ERROR,
  		//		(errcode(ERRCODE_T_R_SERIALIZATION_FAILURE),
    		//		 errmsg("tx %s aborted due to waw", activeTx->hash)));
diff --git a/src/backend/bcdb/worker.c b/src/backend/bcdb/worker.c
index 6e9201e..f1cf07c 100644
--- a/src/backend/bcdb/worker.c
+++ b/src/backend/bcdb/worker.c
@@ -485,6 +485,18 @@ bcdb_worker_process_tx_dt(BCDBShmXact *tx, bool dualTab)
                      activeTx->tx_id_committed);
 #endif
 	  } else {
+	      /* Fix: Clear stale optim_write_list before retry.
+	       * AbortCurrentTransaction will undo heap/index changes,
+	       * but the SIMPLEQ entries allocated in bcdb_tx_context
+	       * need to be freed. MemoryContextReset frees the memory,
+	       * then SIMPLEQ_INIT resets the head pointer. */
+	      while ((optim_write_entry = SIMPLEQ_FIRST(&activeTx->optim_write_list)))
+	      {
+	          if (optim_write_entry->slot)
+	              ExecDropSingleTupleTableSlot(optim_write_entry->slot);
+	          SIMPLEQ_REMOVE_HEAD(&activeTx->optim_write_list, link);
+	      }
+	      MemoryContextReset(bcdb_tx_context);
 	      /* Fix: AbortCurrentTransaction properly releases all resources
 	       * (buffer pins, TupleDesc refs, locks) via ResourceOwnerRelease
 	       * with isCommit=false (silent, no leak warnings).
@@ -492,9 +504,22 @@ bcdb_worker_process_tx_dt(BCDBShmXact *tx, bool dualTab)
 	       * start_xact_command will begin a fresh transaction. */
 	      AbortCurrentTransaction();
 	      reset_xact_command();
-	      if(hold_portal_snapshot) {
-	          hold_portal_snapshot = false;
-	      }
+          if (hold_portal_snapshot && activeTx->portal)
+          {
+              if (activeTx->portal->resowner)
+              {
+                  ResourceOwnerRelease(activeTx->portal->resowner,
+                                       RESOURCE_RELEASE_BEFORE_LOCKS, false, false);
+                  ResourceOwnerRelease(activeTx->portal->resowner,
+                                       RESOURCE_RELEASE_LOCKS, false, false);
+                  ResourceOwnerRelease(activeTx->portal->resowner,
+                                       RESOURCE_RELEASE_AFTER_LOCKS, false, false);
+                  ResourceOwnerDelete(activeTx->portal->resowner);
+                  activeTx->portal->resowner = NULL;
+              }
+              PortalDrop(activeTx->portal, false);
+              hold_portal_snapshot = false;
+          }
 #if SAFEDBG2
               printf("\n\n safeDbg tx %d rerun due to conflict pid %d %s : %s: %d \n",
                      tx->tx_id, getpid(), __FILE__, __FUNCTION__, __LINE__ );
@@ -502,7 +527,7 @@ bcdb_worker_process_tx_dt(BCDBShmXact *tx, bool dualTab)
 	  }
           init = false;
 	  XactIsoLevel = tx->isolation;
-	  start_xact_command();
+      start_xact_command();
 	  tx->status = TX_EXECUTING;
 
 #if SAFEDBG2
@@ -581,9 +606,28 @@ bcdb_worker_process_tx_dt(BCDBShmXact *tx, bool dualTab)
       printf(" safeDbg pid %d %s : %s: %d tx %d \n",
                  getpid(), __FILE__, __FUNCTION__, __LINE__, tx->tx_id );
 #endif
-      set_last_committed_txid(tx);
-
-      apply_optim_writes();
+      /* CRITICAL FIX: Do NOT advance counter before apply_optim_writes.
+       * Old code had set_last_committed_txid(tx) HERE, which woke the
+       * next transaction before our writes were applied/committed,
+       * causing concurrent serial execution and Merkle corruption.
+       * Counter is now advanced AFTER finish_xact_command() below. */
+      if (!apply_optim_writes())
+      {
+          /*
+           * An INSERT failed (e.g. duplicate key because tx_id ordering
+           * didn't match workload line ordering).  Abort this attempt
+           * and retry the whole transaction with a fresh snapshot.
+           *
+           * It's safe to retry even after publish_ws_tableDT because:
+           * 1) No other tx has entered serial yet (counter not advanced)
+           * 2) Our stale published tags won't false-match ourselves
+           *    (table_checkDT checks entry->tx_id < activeTx->tx_id)
+           * 3) AbortCurrentTransaction rolls back partial apply_optim
+           *    writes done before the failure
+           */
+          rw_conflicts = 1;
+          continue;
+      }
       tx->sxact->flags |= SXACT_FLAG_PREPARED;
 
       DEBUGMSG("[ZL] worker(%d) commiting tx(%s)", getpid(), tx->hash);
@@ -623,8 +667,9 @@ bcdb_worker_process_tx_dt(BCDBShmXact *tx, bool dualTab)
     fclose(fp);
     }
 
-      ConditionVariableBroadcast(&block->cond);
-      condSig = 1;
+      /* CRITICAL FIX: Broadcast moved to after finish_xact_command
+       * to prevent next tx from starting serial phase before our
+       * writes are committed. condSig set after broadcast below. */
 
       // For tx "select saveState()" char 't' should be seen by offset 9
       for(sqlOffset = 0; sqlOffset < saveLen ; sqlOffset++) {
@@ -710,14 +755,28 @@ bcdb_worker_process_tx_dt(BCDBShmXact *tx, bool dualTab)
 	  PortalDrop(activeTx->portal, false);
           hold_portal_snapshot = false;
       }
-      /* Also release TopTransactionResourceOwner's direct resources */
+      /* Release TopTransactionResourceOwner's direct resources.
+       * CRITICAL: isTopLevel (4th param) MUST be true since this is a
+       * top-level transaction. Using false triggers Assert(owner->parent != NULL)
+       * at resowner.c:582 during RESOURCE_RELEASE_LOCKS, because it tries
+       * retail lock release on child owners expecting subtransaction semantics.
+       * Standard PG always uses isTopLevel=true for TopTransactionResourceOwner
+       * (see CommitTransaction/AbortTransaction in xact.c). */
       ResourceOwnerRelease(TopTransactionResourceOwner,
-                           RESOURCE_RELEASE_BEFORE_LOCKS, false, false);
+                           RESOURCE_RELEASE_BEFORE_LOCKS, false, true);
       ResourceOwnerRelease(TopTransactionResourceOwner,
-                           RESOURCE_RELEASE_LOCKS, false, false);
+                           RESOURCE_RELEASE_LOCKS, false, true);
       ResourceOwnerRelease(TopTransactionResourceOwner,
-                           RESOURCE_RELEASE_AFTER_LOCKS, false, false);
+                           RESOURCE_RELEASE_AFTER_LOCKS, false, true);
       finish_xact_command();
+
+      /* CRITICAL FIX: Advance counter and broadcast ONLY AFTER commit.
+       * This ensures the next transaction cannot enter its serial phase
+       * until our writes are fully committed and visible. */
+      set_last_committed_txid(tx);
+      ConditionVariableBroadcast(&block->cond);
+      condSig = 1;
+
       memset(&block->result[mem_txid], 0, 1024);
 #if SAFEDBG3
       printf("safeDbg txid= %d mem-txid = %d \n", tx->tx_id, mem_txid);
@@ -778,11 +837,28 @@ bcdb_worker_process_tx_dt(BCDBShmXact *tx, bool dualTab)
               BCBlock     *block2 = get_block_by_id(1, false);
 	      ConditionVariableBroadcast(&block2->cond);
       }
+      if (hold_portal_snapshot && activeTx && activeTx->portal)
+      {
+          if (activeTx->portal->resowner)
+          {
+              ResourceOwnerRelease(activeTx->portal->resowner,
+                                   RESOURCE_RELEASE_BEFORE_LOCKS, false, false);
+              ResourceOwnerRelease(activeTx->portal->resowner,
+                                   RESOURCE_RELEASE_LOCKS, false, false);
+              ResourceOwnerRelease(activeTx->portal->resowner,
+                                   RESOURCE_RELEASE_AFTER_LOCKS, false, false);
+              ResourceOwnerDelete(activeTx->portal->resowner);
+              activeTx->portal->resowner = NULL;
+          }
+          PortalDrop(activeTx->portal, false);
+          hold_portal_snapshot = false;
+      }
       delete_tx(tx);
       MemoryContextReset(bcdb_tx_context);
       while ((optim_write_entry = SIMPLEQ_FIRST(&activeTx->optim_write_list)))
       {
-	   ExecDropSingleTupleTableSlot(optim_write_entry->slot);
+	   if (optim_write_entry->slot)
+	       ExecDropSingleTupleTableSlot(optim_write_entry->slot);
 	   SIMPLEQ_REMOVE_HEAD(&activeTx->optim_write_list, link);
       }
       PG_RE_THROW();
@@ -866,6 +942,8 @@ bcdb_worker_process_tx(BCDBShmXact *tx)
         conflict_check();
         if (timing)
             tx->end_checking_time = bcdb_get_time();
+        /* Note: ignoring apply_optim_writes return value in non-DT path;
+         * the non-DT path uses conflict_check() which is less strict. */
         apply_optim_writes();
         if (timing)
             tx->end_local_copy_time = bcdb_get_time();
diff --git a/src/backend/executor/nodeModifyTable.c b/src/backend/executor/nodeModifyTable.c
index c7a33f5..4c284b1 100644
--- a/src/backend/executor/nodeModifyTable.c
+++ b/src/backend/executor/nodeModifyTable.c
@@ -67,6 +67,48 @@
 #include <bcdb/shm_transaction.h>
 #include <bcdb/globals.h>
 
+/*
+ * bcdb_compute_key_tag - Compute a write-set tag based on primary key values.
+ *
+ * Both INSERT and DELETE for the same primary key must produce the same tag
+ * so that conflict_checkDT detects cross-operation conflicts on the same key.
+ * Without this, INSERT operations are invisible to conflict detection,
+ * allowing DELETE transactions to proceed with stale snapshots where a key
+ * was temporarily absent, leaving phantom hashes in the merkle tree.
+ *
+ * We read the first column (assumed to be the primary key) from the given
+ * slot and use hash_any to compute a stable 32-bit hash.  The hash is split
+ * into blockNumber (upper 16 bits) and offsetNumber (lower 16 bits + 1,
+ * since offset 0 is invalid) inside a PREDICATELOCKTARGETTAG.
+ */
+static void
+bcdb_compute_key_tag(PREDICATELOCKTARGETTAG *tag, Oid relOid,
+                     TupleTableSlot *slot)
+{
+    Datum   keyVal;
+    bool    isNull;
+    uint32  h;
+
+    /* Extract the first column (primary key) from the slot */
+    keyVal = slot_getattr(slot, 1, &isNull);
+    if (isNull)
+        h = 0;
+    else
+    {
+        int32 intKey = DatumGetInt32(keyVal);
+        h = hash_any((unsigned char *) &intKey, sizeof(int32));
+    }
+
+    /*
+     * Pack into tag.  We use a fixed dbOid of 0 (same as existing code)
+     * and the table's relOid.  The hash is split so that the lower 16-bit
+     * field (offsetNumber) is always >= 1 to keep ItemPointer-like validity.
+     */
+    SET_PREDICATELOCKTARGETTAG_TUPLE(*tag, 0, relOid,
+                                     (BlockNumber)(h >> 16),
+                                     (OffsetNumber)((h & 0xFFFF) | 1));
+}
+
 static bool ExecOnConflictUpdate(ModifyTableState *mtstate,
 								 ResultRelInfo *resultRelInfo,
 								 ItemPointer conflictTid,
@@ -85,6 +127,120 @@ static void ExecSetupChildParentMapForSubplan(ModifyTableState *mtstate);
 static TupleConversionMap *tupconv_map_for_subplan(ModifyTableState *node,
 												   int whichplan);
 
+typedef struct MerkleDeleteDelta
+{
+	Oid			indexOid;
+	int			partitionId;
+	MerkleHash	hash;
+} MerkleDeleteDelta;
+
+typedef struct MerkleDeletePlan
+{
+	int			count;
+	MerkleDeleteDelta *items;
+	bool		ready;
+} MerkleDeletePlan;
+
+static MerkleDeletePlan
+CaptureMerkleDeletePlan(Relation heapRel, ItemPointer tupleid)
+{
+	MerkleDeletePlan plan;
+	List       *indexList;
+	ListCell   *lc;
+	TupleTableSlot *slot;
+	MerkleHash  hash;
+	int			maxItems;
+
+	plan.count = 0;
+	plan.items = NULL;
+	plan.ready = false;
+
+	if (!enable_merkle_index)
+	{
+		plan.ready = true;
+		return plan;
+	}
+
+	if (!ItemPointerIsValid(tupleid) ||
+		ItemPointerGetBlockNumberNoCheck(tupleid) == InvalidBlockNumber)
+		return plan;
+
+	slot = table_slot_create(heapRel, NULL);
+	if (!table_tuple_fetch_row_version(heapRel, tupleid, SnapshotSelf, slot))
+	{
+		ExecDropSingleTupleTableSlot(slot);
+		return plan;
+	}
+
+	merkle_compute_slot_hash(heapRel, slot, &hash);
+	if (merkle_hash_is_zero(&hash))
+	{
+		ExecDropSingleTupleTableSlot(slot);
+		plan.ready = true;
+		return plan;
+	}
+
+	indexList = RelationGetIndexList(heapRel);
+	maxItems = list_length(indexList);
+	if (maxItems > 0)
+		plan.items = (MerkleDeleteDelta *) palloc0(sizeof(MerkleDeleteDelta) * maxItems);
+
+	foreach(lc, indexList)
+	{
+		Oid         indexOid = lfirst_oid(lc);
+		Relation    indexRel;
+
+		indexRel = index_open(indexOid, RowExclusiveLock);
+		if (indexRel->rd_rel->relam == MERKLE_AM_OID)
+		{
+			IndexInfo  *indexInfo;
+			Datum       values[INDEX_MAX_KEYS];
+			bool        isnull[INDEX_MAX_KEYS];
+			int         totalLeaves;
+			int         partitionId;
+
+			indexInfo = BuildIndexInfo(indexRel);
+			FormIndexDatum(indexInfo, slot, NULL, values, isnull);
+			merkle_read_meta(indexRel, NULL, NULL, NULL, NULL, &totalLeaves, NULL, NULL);
+			partitionId = merkle_compute_partition_id(values, isnull,
+											 indexInfo->ii_NumIndexKeyAttrs,
+											 RelationGetDescr(indexRel),
+											 totalLeaves);
+
+			plan.items[plan.count].indexOid = indexOid;
+			plan.items[plan.count].partitionId = partitionId;
+			plan.items[plan.count].hash = hash;
+			plan.count++;
+		}
+		index_close(indexRel, RowExclusiveLock);
+	}
+
+	list_free(indexList);
+	ExecDropSingleTupleTableSlot(slot);
+	plan.ready = true;
+	return plan;
+}
+
+static void
+ApplyMerkleDeletePlan(MerkleDeletePlan *plan)
+{
+	int i;
+
+	if (plan == NULL)
+		return;
+
+	for (i = 0; i < plan->count; i++)
+	{
+		Relation indexRel = index_open(plan->items[i].indexOid, RowExclusiveLock);
+		if (indexRel->rd_rel->relam == MERKLE_AM_OID)
+			merkle_update_tree_path(indexRel,
+							plan->items[i].partitionId,
+							&plan->items[i].hash,
+							false);
+		index_close(indexRel, RowExclusiveLock);
+	}
+}
+
 /*
  * ExecDeleteMerkleIndexes - Update merkle indexes before DELETE
  *
@@ -92,7 +248,7 @@ static TupleConversionMap *tupconv_map_for_subplan(ModifyTableState *node,
  * indexes on the table. This must be done BEFORE the row is deleted
  * because we need to read the row to compute its hash.
  */
-static void
+void
 ExecDeleteMerkleIndexes(Relation heapRel, ItemPointer tupleid)
 {
     List       *indexList;
@@ -230,19 +386,18 @@ ExecInsertMerkleIndexes(Relation heapRel, TupleTableSlot *slot)
 {
     List       *indexList;
     ListCell   *lc;
+	MerkleHash  hash;
 
-    /*
-     * Validate slot->tts_tid before processing.
-     * During optimistic writes the tuple is not yet in the heap,
-     * so tts_tid is invalid — skip silently.
-     */
-    if (!ItemPointerIsValid(&slot->tts_tid) ||
-        ItemPointerGetBlockNumberNoCheck(&slot->tts_tid) == InvalidBlockNumber)
+	if (slot == NULL || TTS_EMPTY(slot))
         return;
 
     if (!enable_merkle_index)
         return;
 
+	merkle_compute_slot_hash(heapRel, slot, &hash);
+	if (merkle_hash_is_zero(&hash))
+		return;
+
     indexList = RelationGetIndexList(heapRel);
 
     foreach(lc, indexList)
@@ -257,14 +412,18 @@ ExecInsertMerkleIndexes(Relation heapRel, TupleTableSlot *slot)
             IndexInfo  *indexInfo;
             Datum       values[INDEX_MAX_KEYS];
             bool        isnull[INDEX_MAX_KEYS];
+			int         totalLeaves;
+			int         partitionId;
 
             indexInfo = BuildIndexInfo(indexRel);
 
             FormIndexDatum(indexInfo, slot, NULL, values, isnull);
-
-            index_insert(indexRel, values, isnull,
-                         &slot->tts_tid, heapRel,
-                         UNIQUE_CHECK_NO, indexInfo);
+			merkle_read_meta(indexRel, NULL, NULL, NULL, NULL, &totalLeaves, NULL, NULL);
+			partitionId = merkle_compute_partition_id(values, isnull,
+													 indexInfo->ii_NumIndexKeyAttrs,
+													 RelationGetDescr(indexRel),
+													 totalLeaves);
+			merkle_update_tree_path(indexRel, partitionId, &hash, true);
         }
 
         index_close(indexRel, RowExclusiveLock);
@@ -780,6 +939,19 @@ ExecInsert(ModifyTableState *mtstate,
 		{
 			if (is_bcdb_worker)
 			{
+				/*
+				 * BCDB WORKER: Register INSERT in the write-set using a
+				 * primary-key-based tag.  This ensures conflict_checkDT
+				 * detects DELETE-INSERT conflicts on the same key.
+				 * Without this, a DELETE transaction whose optimistic phase
+				 * found the key absent (between a prior DELETE commit and a
+				 * later INSERT commit) would proceed without retry, leaving
+				 * a phantom merkle hash from the prior INSERT.
+				 */
+				PREDICATELOCKTARGETTAG tag;
+				bcdb_compute_key_tag(&tag,
+									 resultRelationDesc->rd_id, slot);
+				ws_table_reserveDT(&tag);
 				store_optim_insert(slot);
 				//return NULL;
 			}
@@ -965,12 +1137,68 @@ ExecDelete(ModifyTableState *mtstate,
 		 * mode transactions.
 		 */
 ldelete:;
+		/*
+		 * BCDB WORKER: Defer DELETE to serial phase.
+		 * Just like INSERT and UPDATE, store the operation for later
+		 * execution in apply_optim_writes(). This prevents Merkle tree
+		 * XOR-out from happening during the parallel phase, which would
+		 * cause hash corruption due to concurrent unprotected access.
+		 */
+		if (is_bcdb_worker)
+		{
+			PREDICATELOCKTARGETTAG tag;
+			PREDICATELOCKTARGETTAG tid_tag;
+			/*
+			 * DELETE registers TWO write-set tags:
+			 *
+			 * 1. TID-based tag — preserves RAW conflict detection with
+			 *    SELECT operations that read the same physical tuple.
+			 *    SELECT registers TID-based read-set tags via
+			 *    PredicateLockTuple; if we only publish key-hash tags,
+			 *    the conflict check (TID vs key-hash) never matches.
+			 *
+			 * 2. Key-based tag — enables WAW conflict detection with
+			 *    INSERT operations on the same primary key.  INSERT
+			 *    registers a key-hash write-set tag; the key-hash
+			 *    from DELETE matches it for same-key conflicts.
+			 */
+
+			/* Tag 1: TID-based (always available) */
+			SET_PREDICATELOCKTARGETTAG_TUPLE(tid_tag,
+											 0,
+											 resultRelationDesc->rd_id,
+											 ItemPointerGetBlockNumber(tupleid),
+											 ItemPointerGetOffsetNumber(tupleid));
+			ws_table_reserveDT(&tid_tag);
+
+			/* Tag 2: Key-based (read tuple to extract primary key) */
+			{
+				TupleTableSlot *keySlot;
+				keySlot = table_slot_create(resultRelationDesc, NULL);
+				if (table_tuple_fetch_row_version(resultRelationDesc,
+												  tupleid,
+												  estate->es_snapshot,
+												  keySlot))
+				{
+					bcdb_compute_key_tag(&tag,
+										 resultRelationDesc->rd_id,
+										 keySlot);
+					ws_table_reserveDT(&tag);
+				}
+				/* If tuple not readable, TID tag above is still registered */
+				ExecDropSingleTupleTableSlot(keySlot);
+			}
+			store_optim_delete(RelationGetRelid(resultRelationDesc), tupleid);
+			if (canSetTag)
+				(estate->es_processed)++;
+			return NULL;
+		}
 		/*
 		 * Update Merkle indexes BEFORE deleting the tuple.
 		 * We need to do this before the tuple is gone so we can
 		 * read the row data to compute the hash to XOR out.
 		 */
-		ExecDeleteMerkleIndexes(resultRelationDesc, tupleid);
+		MerkleDeletePlan merkleDeletePlan = CaptureMerkleDeletePlan(resultRelationDesc, tupleid);
 	
 		result = table_tuple_delete(resultRelationDesc, tupleid,
 								estate->es_output_cid,
@@ -1018,6 +1246,11 @@ ldelete:;
 					return NULL;
 
 				case TM_Ok:
+					if (!merkleDeletePlan.ready)
+						ereport(ERROR,
+							(errcode(ERRCODE_INTERNAL_ERROR),
+							 errmsg("failed to capture old-row Merkle delta for DELETE")));
+					ApplyMerkleDeletePlan(&merkleDeletePlan);
 					break;
 
 				case TM_Updated:
@@ -1323,6 +1556,10 @@ ExecUpdate(ModifyTableState *mtstate,
 		LockTupleMode lockmode;
 		bool		partition_constraint_failed;
 		bool		update_indexes;
+		MerkleDeletePlan merkleDeletePlan;
+		merkleDeletePlan.count = 0;
+		merkleDeletePlan.items = NULL;
+		merkleDeletePlan.ready = true;
 
 		/*
 		 * Constraints might reference the tableoid column, so (re-)initialize
@@ -1572,12 +1809,7 @@ lreplace:;
 			}
 			else
 			{
-				/*
-				 * Update Merkle indexes: XOR out the OLD row's hash before the update.
-				 * The NEW row's hash will be XOR'd in when ExecInsertIndexTuples
-				 * calls merkleInsert.
-				 */
-				ExecDeleteMerkleIndexes(resultRelationDesc, tupleid);
+				merkleDeletePlan = CaptureMerkleDeletePlan(resultRelationDesc, tupleid);
 
 				result = table_tuple_update(resultRelationDesc, tupleid, slot,
 											estate->es_output_cid,
@@ -1624,6 +1856,11 @@ lreplace:;
 					return NULL;
 
 				case TM_Ok:
+					if (!merkleDeletePlan.ready)
+						ereport(ERROR,
+							(errcode(ERRCODE_INTERNAL_ERROR),
+							 errmsg("failed to capture old-row Merkle delta for UPDATE")));
+					ApplyMerkleDeletePlan(&merkleDeletePlan);
 					break;
 
 				case TM_Updated:
diff --git a/src/backend/tcop/postgres.c b/src/backend/tcop/postgres.c
index 7c7fa4f..a299e95 100644
--- a/src/backend/tcop/postgres.c
+++ b/src/backend/tcop/postgres.c
@@ -4624,7 +4624,26 @@ PostgresMain(int argc, char *argv[],
      else 
        strcpy(query_string2, (const char *) (query_string+10));
 					
-					BCDBShmXact *tx = create_tx(pfx_str, query_string2, pfx_id, bid, XACT_SERIALIZABLE, true ); 
+					BCDBShmXact *tx = NULL;
+					int tx_retry = 0;
+					const int tx_retry_limit = 400;
+
+					while ((tx = create_tx(pfx_str, query_string2, pfx_id, bid,
+										   XACT_SERIALIZABLE, true)) == NULL)
+					{
+						if (tx_retry == 0)
+							elog(WARNING, "create_tx returned NULL for hash %s, retrying", pfx_str);
+
+						CHECK_FOR_INTERRUPTS();
+						pg_usleep(5000L);
+						tx_retry++;
+
+						if (tx_retry >= tx_retry_limit)
+							ereport(ERROR,
+									(errcode(ERRCODE_CONFIGURATION_LIMIT_EXCEEDED),
+									 errmsg("unable to allocate BCDB transaction slot for hash %s after %d retries", pfx_str, tx_retry_limit),
+									 errhint("Increase BCDB tx-pool capacity or reduce client concurrency.")));
+					}
 #if SAFEDBG3
 	printf("safeDbg %s : %s: %d pfx_hash= %s txid= %d q= %s\n", __FILE__, __FUNCTION__, __LINE__ , pfx_str, pfx_id, (query_string2) );
 #endif
diff --git a/src/bin/bcsql/bcsql b/src/bin/bcsql/bcsql
index 1fb4d5a..be2aa81 100755
Binary files a/src/bin/bcsql/bcsql and b/src/bin/bcsql/bcsql differ
diff --git a/src/include/access/heapam.h b/src/include/access/heapam.h
index 9f72378..6a7a790 100644
--- a/src/include/access/heapam.h
+++ b/src/include/access/heapam.h
@@ -112,6 +112,15 @@ typedef enum
 #define HeapScanIsValid(scan) PointerIsValid(scan)
 
 extern void heap_apply_index(Relation relation, TupleTableSlot *slot, bool conflict_check, bool unique_check);
+
+/* Phase constants for heap_apply_index_phase */
+#define HEAP_INDEX_ALL         0   /* process all indexes */
+#define HEAP_INDEX_NO_MERKLE   1   /* skip merkle indexes */
+#define HEAP_INDEX_MERKLE_ONLY 2   /* only merkle indexes */
+
+extern void heap_apply_index_phase(Relation relation, TupleTableSlot *slot,
+								   bool conflict_check, bool unique_check, int phase);
+
 extern TableScanDesc heap_beginscan(Relation relation, Snapshot snapshot,
 									int nkeys, ScanKey key,
 									ParallelTableScanDesc parallel_scan,
diff --git a/src/include/access/merkle.h b/src/include/access/merkle.h
index 5b0b64a..cce5829 100644
--- a/src/include/access/merkle.h
+++ b/src/include/access/merkle.h
@@ -183,6 +183,8 @@ extern void merkleCostEstimate(struct PlannerInfo *root,
  */
 extern void merkle_compute_row_hash(Relation heapRel, ItemPointer tid,
                                     MerkleHash *result);
+extern void merkle_compute_slot_hash(Relation heapRel, TupleTableSlot *slot,
+                                     MerkleHash *result);
 extern int  merkle_compute_partition_id(Datum *values, bool *isnull,
                                         int nkeys, TupleDesc tupdesc,
                                         int numLeaves);
diff --git a/src/include/bcdb/shm_transaction.h b/src/include/bcdb/shm_transaction.h
index fedee8b..9adb634 100644
--- a/src/include/bcdb/shm_transaction.h
+++ b/src/include/bcdb/shm_transaction.h
@@ -81,6 +81,7 @@ typedef struct _OptimWriteEntry
     CmdType         operation;
     TupleTableSlot  *slot;
     ItemPointerData old_tid;
+    Oid             relOid;     /* relation OID, used for CMD_DELETE */
     CommandId       cid;
     SIMPLEQ_ENTRY(_OptimWriteEntry) link;
 } OptimWriteEntry;
@@ -211,9 +212,11 @@ extern BCDBShmXact* get_tx_by_xid_locked(TransactionId xid, bool exclusive);
 extern uint32 dummy_hash(const void *key, Size key_size);
 extern void store_optim_update(TupleTableSlot* slot, ItemPointer old_tid);
 extern void store_optim_insert(TupleTableSlot* slot);
+extern void store_optim_delete(Oid relOid, ItemPointer tupleid);
 extern void apply_optim_update(ItemPointer tid, TupleTableSlot* slot, CommandId cid);
-extern void apply_optim_insert(TupleTableSlot* slot, CommandId cid);
-extern void apply_optim_writes(void);
+extern bool apply_optim_insert(TupleTableSlot* slot, CommandId cid);
+extern void apply_optim_delete(Oid relOid, ItemPointer tupleid, CommandId cid);
+extern bool apply_optim_writes(void);
 extern bool check_stale_read(void);
 extern void clean_ws_table_record(void);
 extern bool rs_table_check(PREDICATELOCKTARGETTAG *tag);
diff --git a/src/include/executor/nodeModifyTable.h b/src/include/executor/nodeModifyTable.h
index 6342f07..c33afa0 100644
--- a/src/include/executor/nodeModifyTable.h
+++ b/src/include/executor/nodeModifyTable.h
@@ -18,6 +18,7 @@
 
 extern void ExecComputeStoredGenerated(EState *estate, TupleTableSlot *slot);
 extern void ExecInsertMerkleIndexes(Relation heapRel, TupleTableSlot *slot);
+extern void ExecDeleteMerkleIndexes(Relation heapRel, ItemPointer tupleid);
 
 extern ModifyTableState *ExecInitModifyTable(ModifyTable *node, EState *estate, int eflags);
 extern void ExecEndModifyTable(ModifyTableState *node);
